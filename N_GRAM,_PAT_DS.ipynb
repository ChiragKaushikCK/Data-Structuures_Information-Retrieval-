{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxlpQpT22BhS",
        "outputId": "2be88d63-aab6-4424-b4cf-2ecbe2c0b552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated N-Grams with Counts:\n",
            "('this', 'is', 'a'): 2\n",
            "('is', 'a', 'simple'): 1\n",
            "('a', 'simple', 'test'): 1\n",
            "('simple', 'test', 'sentence'): 1\n",
            "('test', 'sentence', 'for'): 1\n",
            "('sentence', 'for', 'n-grams'): 1\n",
            "('for', 'n-grams', 'this'): 1\n",
            "('n-grams', 'this', 'is'): 1\n",
            "('is', 'a', 'test'): 1\n"
          ]
        }
      ],
      "source": [
        "# NGRAM\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "class NGram:\n",
        "    def __init__(self, n):\n",
        "        self.n = n\n",
        "        self.ngrams = Counter()\n",
        "\n",
        "    def generate_ngrams(self, text):\n",
        "        tokens = word_tokenize(text)\n",
        "        self.ngrams.update(ngrams(tokens, self.n))\n",
        "\n",
        "    def get_ngrams(self):\n",
        "        return self.ngrams\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    import nltk\n",
        "    nltk.download('punkt')  # Download tokenizer resources\n",
        "\n",
        "    ngram_model = NGram(3)  # Create a trigram model\n",
        "    text = \"this is a simple test sentence for n-grams this is a test\"\n",
        "    ngram_model.generate_ngrams(text)\n",
        "\n",
        "    print(\"Generated N-Grams with Counts:\")\n",
        "    for ngram, count in ngram_model.get_ngrams().items():\n",
        "        print(f\"{ngram}: {count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PAT\n",
        "!pip install datrie\n",
        "import datrie\n",
        "import string\n",
        "\n",
        "# Initialize the PAT trie (PATRICIA Trie) to store strings made of ASCII characters\n",
        "trie = datrie.Trie(string.ascii_lowercase)\n",
        "\n",
        "# Insert some example strings into the PAT trie\n",
        "trie['hello'] = 1\n",
        "trie['hell'] = 2\n",
        "trie['he'] = 3\n",
        "trie['hero'] = 4\n",
        "trie['her'] = 5\n",
        "\n",
        "# Search for a prefix in the trie\n",
        "print(\"Words starting with 'he':\", trie.keys('he'))\n",
        "\n",
        "# Access specific words\n",
        "print(\"Value associated with 'hello':\", trie['hello'])\n",
        "\n",
        "# Remove a word\n",
        "del trie['hero']\n",
        "print(\"After deleting 'hero', words starting with 'he':\", trie.keys('he'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Inverted file Structure\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "class InvertedIndex:\n",
        "    def __init__(self):\n",
        "        self.index = defaultdict(list)\n",
        "\n",
        "    def build_index(self, documents):\n",
        "        for doc_id, text in enumerate(documents):\n",
        "            for word in word_tokenize(text.lower()):\n",
        "                self.index[word].append(doc_id)\n",
        "\n",
        "    def get_index(self):\n",
        "        return self.index\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    import nltk\n",
        "    nltk.download('punkt')  # Download tokenizer resources\n",
        "\n",
        "    documents = [\n",
        "        \"Inverted index is useful in search engines\",\n",
        "        \"Search engines use inverted index\",\n",
        "        \"An index maps words to documents\"\n",
        "    ]\n",
        "\n",
        "    inverted_index = InvertedIndex()\n",
        "    inverted_index.build_index(documents)\n",
        "\n",
        "    for word, doc_ids in inverted_index.get_index().items():\n",
        "        print(f\"{word}: {doc_ids}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYXgLjb-BI1C",
        "outputId": "e3cf8bcc-e76b-40f2-d2a6-b4b0e1c1db7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datrie\n",
            "  Downloading datrie-0.8.2.tar.gz (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m51.2/63.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m61.4/63.3 kB\u001b[0m \u001b[31m870.8 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 kB\u001b[0m \u001b[31m665.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: datrie\n",
            "  Building wheel for datrie (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for datrie: filename=datrie-0.8.2-cp310-cp310-linux_x86_64.whl size=746969 sha256=95020744e71c19e4b070939f75458289c0d5052482ca36771293f3c99c246158\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/a9/ac/b8e50d96214c3576ab73ac30d0a9b276014f06036f36230e7e\n",
            "Successfully built datrie\n",
            "Installing collected packages: datrie\n",
            "Successfully installed datrie-0.8.2\n",
            "Words starting with 'he': ['he', 'hell', 'hello', 'her', 'hero']\n",
            "Value associated with 'hello': 1\n",
            "After deleting 'hero', words starting with 'he': ['he', 'hell', 'hello', 'her']\n",
            "inverted: [0, 1]\n",
            "index: [0, 1, 2]\n",
            "is: [0]\n",
            "useful: [0]\n",
            "in: [0]\n",
            "search: [0, 1]\n",
            "engines: [0, 1]\n",
            "use: [1]\n",
            "an: [2]\n",
            "maps: [2]\n",
            "words: [2]\n",
            "to: [2]\n",
            "documents: [2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7RFOwhKlGtci"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}